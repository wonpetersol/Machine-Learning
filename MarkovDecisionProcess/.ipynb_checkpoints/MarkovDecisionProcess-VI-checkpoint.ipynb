{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS7641 Machine Learning Homework 4 - Markov Decision Process - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time, pickle, os\n",
    "\n",
    "# Data Related Modules\n",
    "import pandas as pd\n",
    "\n",
    "# Import tools for MDP\n",
    "import tools\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "← ↓ → ↑\n"
     ]
    }
   ],
   "source": [
    "# action mapping for display the final result\n",
    "action_mapping = {\n",
    "    3: '\\u2191', # UP\n",
    "    2: '\\u2192', # RIGHT\n",
    "    1: '\\u2193', # DOWN\n",
    "    0: '\\u2190' # LEFT\n",
    "}\n",
    "print(' '.join([action_mapping[i] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(enviorment, n_episodes, policy, random = False):\n",
    "    # intialize wins and total reward\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # loop over number of episodes to play\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        # flag to check if the game is finished\n",
    "        terminated = False\n",
    "        \n",
    "        # reset the enviorment every time when playing a new episode\n",
    "        state = enviorment.reset()\n",
    "        \n",
    "        while not terminated:\n",
    "            \n",
    "            # check if the random flag is not true then follow the given policy other wise take random action\n",
    "            if random:\n",
    "                action = enviorment.action_space.sample()\n",
    "            else:\n",
    "                action = policy[state]\n",
    "\n",
    "            # take the next step\n",
    "            next_state, reward,  terminated, info = enviorment.step(action)\n",
    "            \n",
    "            enviorment.render()\n",
    "            \n",
    "            # accumalate total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # change the state\n",
    "            state = next_state\n",
    "            \n",
    "            # if game is over with positive reward then add 1.0 in wins\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "                \n",
    "    # calculate average reward\n",
    "    average_reward = total_reward / n_episodes\n",
    "    \n",
    "    return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
    "    \"\"\"\n",
    "    Helper function to  calculate state-value function\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM Enviorment object\n",
    "        state: state to consider\n",
    "        V: Estimated Value for each state. Vector of length nS\n",
    "        discount_factor: MDP discount factor\n",
    "        \n",
    "    Return:\n",
    "        action_values: Expected value of each action in a state. Vector of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vector of action values\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    # loop over the actions we can take in an enviorment \n",
    "    for action in range(env.nA):\n",
    "        # loop over the P_sa distribution.\n",
    "        for probablity, next_state, reward, info in env.P[state][action]:\n",
    "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
    "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(env, policy, V, discount_factor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to update a given policy based on given value function.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM Enviorment object.\n",
    "        policy: policy to update.\n",
    "        V: Estimated Value for each state. Vector of length nS.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy: Updated policy based on the given state-Value function 'V'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        # for a given state compute state-action value.\n",
    "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "        \n",
    "        # choose the action which maximizez the state-action value.\n",
    "        policy[state] =  np.argmax(action_values)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "    Algorithm to solve MPD.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM Enviorment object.\n",
    "        discount_factor: MDP discount factor.\n",
    "        max_iteration: Maximum No.  of iterations to run.\n",
    "        \n",
    "    Return:\n",
    "        V: Optimal state-Value function. Vector of lenth nS.\n",
    "        optimal_policy: Optimal policy. Vector of length nS.\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize value fucntion\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    # iterate over max_iterations\n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        #  keep track of change with previous value function\n",
    "        prev_v = np.copy(V) \n",
    "    \n",
    "        # loop over all states\n",
    "        for state in range(env.nS):\n",
    "            \n",
    "            # Asynchronously update the state-action value\n",
    "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "            \n",
    "            # Synchronously update the state-action value\n",
    "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
    "            \n",
    "            # select best action to perform based on highest state-action value\n",
    "            best_action_value = np.max(action_values)\n",
    "            \n",
    "            # update the current state-value fucntion\n",
    "            V[state] =  best_action_value\n",
    "            \n",
    "        # if policy not changed over 10 iterations it converged.\n",
    "        if i % 10 == 0:\n",
    "            # if values of 'V' not changing after one iteration\n",
    "            if (np.all(np.isclose(V, prev_v))):\n",
    "                print('Value converged at iteration %d' %(i+1))\n",
    "                break\n",
    "\n",
    "    # intialize optimal policy\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    \n",
    "    # update the optimal polciy according to optimal value function 'V'\n",
    "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
    "    \n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value converged at iteration 341\n",
      "Time to converge:  81.3 ms\n",
      "Optimal Value function: \n",
      "[[0.78538826 0.77836049 0.77368481 0.7713498 ]\n",
      " [0.78775777 0.         0.50562724 0.        ]\n",
      " [0.79250312 0.79963699 0.74472318 0.        ]\n",
      " [0.         0.86409247 0.93114742 0.        ]]\n",
      "Final Policy: \n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n"
     ]
    }
   ],
   "source": [
    "enviorment = gym.make('FrozenLake-v0')\n",
    "tic = time.time()\n",
    "opt_V, opt_Policy = value_iteration(enviorment.env, max_iteration = 1000)\n",
    "toc = time.time()\n",
    "elapsed_time = (toc - tic) * 1000\n",
    "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
    "print('Optimal Value function: ')\n",
    "print(opt_V.reshape((4, 4)))\n",
    "print('Final Policy: ')\n",
    "print(opt_Policy)\n",
    "print(' '.join([action_mapping[int(action)] for action in opt_Policy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
